import numpy as np
from .embedder import VectorDB, processed_promptmd5
from .utils import read_problem
from tqdm.auto import tqdm
import gradio as gr
import json
import asyncio
from fastapi import FastAPI
from fastapi.responses import HTMLResponse, FileResponse
import uvicorn
import urllib
import time
import requests
from concurrent.futures import ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=8)

db = VectorDB()
db.load_all()
print("read", len(set(x[0] for x in db.metadata)), "problems")
print(db.metadata[:100])

with open("settings_sample.json") as f:
    settings = json.load(f)

# åˆ›å»ºè‡ªå®šä¹‰APIå®¢æˆ·ç«¯ç±»
class SiliconFlowClient:
    def __init__(self, api_key):
        self.api_key = "Bearer "+settings["Bearer_API_KEY"]
        self.url = "https://api.siliconflow.cn/v1/chat/completions"
        self.headers = {
            "Authorization": "Bearer "+settings["Bearer_API_KEY"],
            "Content-Type": "application/json"
        }
    
    async def chat_completion(self, messages, model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", temperature=0.7):
        payload = {
            "model": model,
            "stream": False,
            "messages": messages,
            "max_tokens": 1024,
            "temperature": temperature,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1
        }
        # ä½¿ç”¨å¼‚æ­¥è¯·æ±‚
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None,
            lambda: requests.post(self.url, json=payload, headers=self.headers)
        )
        
        if response.status_code != 200:
            raise Exception(f"Error: {response.status_code}, {response.text}")
        
        result = response.json()
        # æ¨¡æ‹ŸOpenAIå“åº”ç»“æ„
        return type('obj', (object,), {
            'choices': [
                type('obj', (object,), {
                    'message': type('obj', (object,), {
                        'content': result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    })
                })
            ]
        })

    # æ·»åŠ åµŒå…¥APIæ–¹æ³•ï¼Œæ›¿ä»£Voyage API
    # æ›¿æ¢SiliconFlowClientçš„embedæ–¹æ³•
    async def embed(self, texts, model=None, input_type=None):
        """ä½¿ç”¨SiliconFlow APIè®¡ç®—æ–‡æœ¬åµŒå…¥"""
        if not texts:
            return type('obj', (object,), {
                'embeddings': [],
                'total_tokens': 0
            })
        
        # ä½¿ç”¨æ–‡æœ¬åµŒå…¥API - æŒ‰ç…§æ­£ç¡®æ ¼å¼æ„å»ºè¯·æ±‚
        payload = {
            "input": texts,
            "encoding_format": "float",
            "model": "BAAI/bge-large-en-v1.5"  
        }
        
        url = "https://api.siliconflow.cn/v1/embeddings"
        headers = {
            "Authorization": "Bearer "+settings["Bearer_API_KEY"],
            "Content-Type": "application/json"
        }
        
        loop = asyncio.get_running_loop()
        try:
            response = await loop.run_in_executor(
                None,
                lambda: requests.post(url, json=payload, headers=headers)
            )
            
            if response.status_code != 200:
                print(f"åµŒå…¥APIé”™è¯¯: {response.status_code}, {response.text}")
                # è¿”å›ç©ºåµŒå…¥ä»¥é˜²æ­¢ç¨‹åºå´©æºƒ
                return type('obj', (object,), {
                    'embeddings': [np.random.randn(1024).tolist() for _ in texts],  # ä½¿ç”¨éšæœºåµŒå…¥
                    'total_tokens': 0
                })
            
            result = response.json()
            print("åµŒå…¥APIå“åº”:", result.keys())
            
            # æ­£ç¡®è§£æSiliconFlow APIçš„å“åº”ç»“æ„
            embeddings = []
            for item in result.get('data', []):
                if 'embedding' in item:
                    embeddings.append(item['embedding'])
            
            # æ¨¡æ‹ŸVoyageå“åº”ç»“æ„
            return type('obj', (object,), {
                'embeddings': embeddings,
                'total_tokens': result.get('usage', {}).get('total_tokens', 0)
            })
        
        except Exception as e:
            print(f"åµŒå…¥APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
            # è¿”å›ç©ºåµŒå…¥ä»¥é˜²æ­¢ç¨‹åºå´©æºƒ
            return type('obj', (object,), {
                'embeddings': [np.random.randn(1024).tolist() for _ in texts],  # ä½¿ç”¨éšæœºåµŒå…¥
                'total_tokens': 0
            })

# ä½¿ç”¨å•ä¸€å®¢æˆ·ç«¯æ›¿æ¢æ‰€æœ‰APIå®¢æˆ·ç«¯
silicon_client = SiliconFlowClient(api_key=settings.get('SILICON_API_KEY', ''))

async def querier(statement, *template_choices):
    assert len(template_choices) % 3 == 0
    yields = []
    ORIGINAL = statement.strip()
    t1 = time.time()

    async def process_template(engine, prompt, prefix):
        if 'origin' in engine.lower() or 'ä¿' in engine.lower():
            return ORIGINAL
        if 'none' in engine.lower() or 'è·³' in engine.lower():
            return ''

        prompt = prompt.replace("[[ORIGINAL]]", ORIGINAL).strip()
        
        # æ‰€æœ‰LLMè¯·æ±‚éƒ½é€šè¿‡SiliconFlowå®¢æˆ·ç«¯å¤„ç†
        if "deepseek" in engine.lower() or "gemma" in engine.lower() or "gpt" in engine.lower():
            response = await silicon_client.chat_completion(
                messages=[
                    {"role": "system", "content": "You are a helpful assistant.answer my questions in English."}, 
                    {"role": "user", "content": prompt},
                    # å¯¹äºéœ€è¦å‰ç¼€çš„æ¨¡å‹ï¼Œæ·»åŠ åŠ©æ‰‹å‰ç¼€
                    {"role": "assistant", "content": prefix} if any(m in engine.lower() for m in ["gemma", "deepseek"]) else None
                ],
                model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            )
            result = response.choices[0].message.content.strip()
            # å¯¹äºGPTæ¨¡å‹ï¼Œç§»é™¤å‰ç¼€
            if "gpt" in engine.lower():
                result = result.replace(prefix.strip(), '', 1).strip()
            return result
        else:
            raise NotImplementedError(engine)

    tasks = [process_template(template_choices[i], template_choices[i+1], template_choices[i+2]) 
             for i in range(0, len(template_choices), 3)]
    yields = await asyncio.gather(*tasks)

    t2 = time.time()
    print('query llm', t2-t1)
    
    # ä½¿ç”¨åµŒå…¥API
    response = await silicon_client.embed(
        list(set(y.strip() for y in yields if len(y))),
        #model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
        model="BAAI/bge-large-en-v1.5"
    )
    
    print('Token spent', response.total_tokens)
    emb = [d for d in response.embeddings]
    t3 = time.time()
    print('query emb', t3-t2)

    loop = asyncio.get_running_loop()
    nearest = await loop.run_in_executor(executor, db.query_nearest, emb, 5000)
    t4 = time.time()
    print('query nearest', t4-t3)

    sim = np.array([x[0] for x in nearest])
    ids = np.array([x[1] for x in nearest], dtype=np.int32)

    info = 'å·²æŸ¥æ‰¾åˆ°å‰' + str(len(sim)) + 'ä¸ªåŒ¹é…ï¼è¿›å…¥ä¸‹ä¸€é¡µæŸ¥çœ‹ç»“æœ~'

    return [info, (sim, ids)] + yields



def format_problem(uid, sim):
    # è·å–é—®é¢˜æ•°æ®
    uid = db.metadata[int(uid)][0]
    problem = read_problem(uid)
    statement = problem["statement"].replace("\n", "\n\n")
    title = problem['title']
    
    # åˆ›å»ºHTMLå±•ç¤º
    html = f'<p><span style="font-size:22px; font-weight: 500;">{title}</span>&nbsp;&nbsp;<span style="font-size:15px">({round(sim*100)}%)</span></p>\n'
    url = problem["url"]
    problemlink = uid.replace('/',' ').replace('\\',' ').strip().replace('problems vjudge','',1).strip().replace('_','-')
    assert problemlink.endswith('.json')
    problemlink = problemlink[:-5].strip()
    
    # æœç´¢é“¾æ¥
    link1 = 'https://www.google.com/search?'+urllib.parse.urlencode({'q': problem['source']+' '+title})
    link1_bd = 'https://www.baidu.com/s?'+urllib.parse.urlencode({'wd': problem['source']+' '+title})
    
    html += f'&nbsp;&nbsp;&nbsp;<a href="{url}" target="_blank">VJudge</a>&nbsp;&nbsp;<a href="{link1}" target="_blank">è°·æ­Œ</a>&nbsp;&nbsp;<a href="{link1_bd}" target="_blank">ç™¾åº¦</a>'
    
    # å¤„ç†é¢˜ç›®ç®€è¦
    markdown = ''
    rsts = []
    for template in settings.get('TEMPLATES', settings['TEMPLATES']):
        md5 = processed_promptmd5(problem['statement'], template)
        rst = None
        for t in problem.get("processed",[]):
            if t["prompt_md5"][:8] == md5:
                rst = t["result"]
        if rst is not None:
            rsts.append(rst)
    
    rsts.sort(key=len)
    for idx, rst in enumerate(rsts):
        markdown += f'### ç®€è¦é¢˜æ„ {idx+1}\n\n{rst}\n\n'
    
    if markdown != '':
        markdown += '<br/>\n\n'
    
    markdown += f'### åŸå§‹é¢˜é¢\n\n```python\n{statement}\n```'
    return html, markdown
# åˆ›å»ºå•ä¸€è¯­è¨€ç•Œé¢
def create_interface():
    with gr.Blocks(
        title="CTFcryptoåŸé¢˜æœº", css="""
        .mymarkdown {font-size: 15px !important}
        footer{display:none !important}
        .centermarkdown{text-align:center !important}
        .pagedisp{text-align:center !important; font-size: 20px !important}
        .realfooter{color: #888 !important; font-size: 14px !important; text-align: center !important;}
        .realfooter a{color: #888 !important;}
        .smallbutton {min-width: 30px !important;}
        """
    ) as demo:
        gr.Markdown("""
# åŸé¢˜æœº
åŸé¢˜åœ¨å“ªé‡Œå•Šï¼ŒåŸé¢˜åœ¨è¿™é‡Œ~"""
        )
        with gr.Tabs() as tabs:
            with gr.TabItem("æœç´¢", id=0):
                input_text = gr.TextArea(
                    label="é¢˜ç›®æè¿°",
                    info="åœ¨è¿™é‡Œç²˜è´´ä½ è¦æœç´¢çš„é¢˜ç›®ï¼",
                    value="m = pow(c,d,n)",
                )
                bundles = []
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                    gr.Markdown("è¾“å…¥çš„é—®é¢˜æè¿°å°†è¢«é‡å†™å¹¶è®¡ç®—ä¸æ¯ä¸ªåŸé—®é¢˜çš„æœ€å¤§ç›¸ä¼¼åº¦ã€‚")
                    for template_id in range(1): # å‡å°‘æ¨¡æ¿æ•°é‡ä¸º2ä¸ª
                        with gr.Accordion("ç‰ˆæœ¬ "+str(template_id+1)):
                            with gr.Row():
                                with gr.Group():
                                    template = settings.get('TEMPLATES', settings['TEMPLATES'])[template_id]
                                    engines = ["ä¿ç•™åŸæè¿°", "deepseek-R1-32B"]
                                    engine = gr.Radio(
                                        engines,
                                        label="ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹",
                                        value=engines[1],
                                        interactive=True,
                                    )
                                    prompt = gr.TextArea(
                                        label="æç¤ºè¯ ([[ORIGINAL]] å°†è¢«æ›¿æ¢ä¸ºé—®é¢˜æè¿°)",
                                        value=template,
                                        interactive=True,
                                        visible=True,
                                    )
                                    prefix = gr.Textbox(
                                        label="å›å¤å‰ç¼€",
                                        value="Simplified statement:",
                                        interactive=True,
                                        visible=True,
                                    )
                                    # æ§åˆ¶å¯è§æ€§
                                    engine.change(lambda engine: (gr.update(visible=not any(s in engine.lower() for s in ['ä¿ç•™', 'è·³è¿‡'])),)*2, engine, [prompt, prefix])
                                output_text = gr.TextArea(
                                    label="é‡å†™ç»“æœ",
                                    value="",
                                    interactive=False,
                                )
                        bundles.append((engine, prompt, prefix, output_text))
                search_result = gr.State(([],[]))
                submit_button = gr.Button("æœç´¢ï¼")
                status_text = gr.Markdown("", elem_classes="centermarkdown")
            with gr.TabItem("æŸ¥çœ‹ç»“æœ", id=1):
                cur_idx = gr.State(0)
                num_columns = gr.State(1)
                
                # # è·å–OJåˆ—è¡¨ ctfçš„æ¯”èµ›åç§°ç®€ç›´æ˜¯åœ°ç‹±ç»˜å›¾ã€‚
                # if hasattr(db, 'sources') and db.sources:
                #     ojs = [f'{t} ({c})' for t,c in sorted(db.sources.items())]
                # else:
                #     ojs = []
                
                # oj_dropdown = gr.Dropdown(
                #     ojs, value=ojs, multiselect=True, label="å±•ç¤ºçš„OJ",
                #     info="ä¸åœ¨è¿™ä¸ªåˆ—è¡¨é‡Œçš„OJçš„é¢˜ç›®å°†è¢«å¿½ç•¥ã€‚å¯ä»¥åœ¨è¿™é‡Œåˆ æ‰ä½ ä¸è®¤è¯†çš„OJã€‚",
                # )
                
                # é‡ç½®ç´¢å¼•
                #oj_dropdown.change(lambda: 0, None, cur_idx)
                statement_min_len = gr.Slider(
                    minimum=1,
                    maximum=2000,
                    label="æœ€å°é¢˜é¢é•¿åº¦",
                    value=20,
                    info="å»é™¤æ•°å­—å’Œç©ºç™½å­—ç¬¦åé¢˜é¢é•¿åº¦å°äºè¯¥å€¼çš„é¢˜ç›®å°†è¢«å¿½ç•¥ã€‚å¯ä»¥ç”¨æ¥ç­›æ‰ä¸€äº›å¥‡æ€ªçš„é¢˜é¢ã€‚",
                )

                with gr.Row():
                    add_column = gr.Button("+", elem_classes='smallbutton')
                    prev_page = gr.Button("â†", elem_classes='smallbutton')
                    home_page = gr.Button("H", elem_classes='smallbutton')
                    next_page = gr.Button("â†’", elem_classes='smallbutton')
                    remove_column = gr.Button("-", elem_classes='smallbutton')
                    
                    # é¡µé¢å¯¼èˆªåŠŸèƒ½
                    prev_page.click(lambda cur_idx, num_columns: max(cur_idx - num_columns, 0), [cur_idx, num_columns], cur_idx, concurrency_limit=None)
                    next_page.click(lambda cur_idx, num_columns: cur_idx + num_columns, [cur_idx, num_columns], cur_idx, concurrency_limit=None)
                    home_page.click(lambda: 0, None, cur_idx, concurrency_limit=None)
                    
                    def adj_idx(idx, col):
                        return int(round(idx / col)) * col
                        
                    add_column.click(lambda cur_idx, num_columns: (adj_idx(cur_idx, num_columns + 1), num_columns + 1), [cur_idx, num_columns], [cur_idx, num_columns], concurrency_limit=None)
                    remove_column.click(lambda cur_idx, num_columns: (adj_idx(cur_idx, num_columns - 1), num_columns - 1) if num_columns >1 else (cur_idx, num_columns), [cur_idx, num_columns], [cur_idx, num_columns], concurrency_limit=None)

                @gr.render(inputs=[search_result,  cur_idx, num_columns, statement_min_len], concurrency_limit=None)
                def show_OJs(search_result, cur_idx, num_columns, statement_min_len):
                    
                    gr.Markdown(f'ç¬¬ {round(cur_idx/num_columns)+1} é¡µ / å…± {'2000+'} é¡µ (æ¯é¡µæ˜¾ç¤º {num_columns} ä¸ª)',
                               elem_classes="pagedisp")
                    cnt = 0
                    with gr.Row():
                        for sim, idx in zip(search_result[0], search_result[1]):
                            if not hasattr(db, 'metadata') or len(db.metadata) <= idx:
                                continue
                            if len(db.metadata[idx]) < 3 or db.metadata[idx][2] < statement_min_len:  # ç§»é™¤OJè¿‡æ»¤æ¡ä»¶
                                continue
                            cnt += 1
                            if cur_idx+1 <= cnt:
                                if cnt > cur_idx+num_columns: break
                                with gr.Column(variant='compact'):
                                    html, md = format_problem(idx, sim)
                                    gr.HTML(html)
                                    gr.Markdown(
                                        latex_delimiters=[
                                            {"left": "$$", "right": "$$", "display": True},
                                            {"left": "$", "right": "$", "display": False},
                                            {"left": "\\(", "right": "\\)", "display": False},
                                            {"left": "\\[", "right": "\\]", "display": True},
                                        ],
                                        value=md,
                                        elem_classes="mymarkdown",
                                    )

        # é¡µè„š
        gr.HTML(
            """<div class="realfooter">Adapted from <a href="https://github.com/fjzzq2002/is-my-problem-new">https://github.com/fjzzq2002/is-my-problem-new</a></div><br /><div class="realfooter">Built with ğŸ’¦ by <a href="https://github.com/LiErJ3412">LiErJ</a></div> """
        )
        
        # æœç´¢åŠŸèƒ½
        async def async_querier_wrapper(*args):
            result = await querier(*args)
            return (gr.Tabs(selected=1),) + tuple(result)
        
        submit_button.click(
            fn=async_querier_wrapper,
            inputs=sum([list(t[:-1]) for t in bundles], [input_text]),
            outputs=[tabs, status_text, search_result] + [t[-1] for t in bundles],
            concurrency_limit=7,
        )
    return demo

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()
favicon_path = 'favicon.ico'

@app.get('/favicon.ico', include_in_schema=False)
async def favicon():
    return FileResponse(favicon_path)

# æŒ‚è½½Gradioåº”ç”¨
app = gr.mount_gradio_app(app, create_interface(), path="/")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)  # æ”¹ä¸º8000ç«¯å£